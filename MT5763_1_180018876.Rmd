---
title: "Bike Share Analysis"
author: "Matthew Coudert"
date: "Saturday September 26th"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
Bike Shares have been rising in popularity over the past few years in many cities all around the world. In order to predict usage of bikes throughout the year, we will explore the relationships between bike usage and other variables, such as day of the year, season, temperature and other meteorlogical variables. After exploring these relationships, we will use this knowledge to build a model in order to predict bike demand throughout the year. For this project, we have data from Washington DC and Seoul, and will use these data to build two distinct models. 

## Downloading Packages Needed
In order to assist us with the data visualization and clean up, we will download 3 packages:

1. **tidyverse**: Assists with putting the data into 'tidy' format
2. **ggplot2**: Makes plotting cleaner and more readable, as well as adding functionality
3. **lubridate**: Makes dates easier to work with in our datasets
```{r packages}
library(tidyverse)
library(ggplot2)
library(lubridate)
```

# Data Cleanup
In order to do analysis on the data, we first need to 'tidy' the data in order to have it in a format that will be easier to work with as well as being in the same format across both datasets for comparison. In order to accomplish this we will put the data into 'tidy' format using the package tidyverse. In order for data to be 'tidy', it must have: "Rows containing differrent observations; columns containing different variables and cells containing values."


## Saving datasets
The messy datasets are saved in the csv format for easy parsing. 
```{r datasets, message = FALSE, warning = FALSE, results = FALSE}
washington_dataset <- read_csv("BikeWashingtonDC.csv")
seoul_dataset <- read_csv("BikeSeoul.csv")
```

## Seoul dataset clean up
This is a lot! I'll go line by line in order to explain what I'm doing. 

1. Piping the dataset into the next variable
2. Selecting only the variables we are going to use in our analysis
3. Removing the times that there is no count within the dataset
4. Renaming the variables in order to be consistent across the two datasets and have a consistent PascalCase naming convention.
5. Save the date as a lubridate date format so it is easier to work with in the analysis
6. Creating a new date variable FullDate that stores both the date and the time of day
7. FullDate eliminates the need to have seperate variables for Date and Hour, so remove these
8. Changing the Holiday variable from a Character type to a Factor with levels "Yes" and "No"
9. Changing the Season variable from a Character type to an Ordered Factor with levels "Spring","Summer","Autumn", and "Winter" in that order. 
```{r seoul_dataset}
seoul_dataset <- seoul_dataset %>%
  filter(seoul_dataset$`Functioning Day`=="Yes") %>%
  select(-c('Visibility (10m)','Dew point temperature(C)','Solar Radiation (MJ/m2)','Rainfall(mm)','Snowfall (cm)','Functioning Day')) %>% 
  filter(!is.na('Rented Bike Count')) %>%
  rename(Date = 'Date',Count = 'Rented Bike Count',Hour = 'Hour', Temperature = 'Temperature(C)', Humidity = 'Humidity(%)', WindSpeed = 'Wind speed (m/s)', Season = 'Seasons', Holiday = 'Holiday') %>%
  mutate(Date = parse_date_time(Date,"dmy")) %>%
  mutate(FullDate = make_datetime(year = year(Date), month = month(Date), day = day(Date), hour = Hour)) %>%
  select(-c(Date,Hour)) %>%
  mutate(Holiday = factor(ifelse(Holiday == "Holiday", "Yes", "No"), levels = c("Yes","No"))) %>%
  mutate(Season = ordered(Season, levels = c("Spring","Summer","Autumn","Winter")))
```

## Washington DC dataset clean up
Similar to the one above I'll go line by line to explain what's going on!

1. Again piping the dataset into the next variables
2. Selecting and renaming the variables we're using for analysis for consistency. 
3. Removing the values for which there is no count
4. Changing humidity into a percentage format to be consistent with the Seoul dataset. ($=\text{humidity}*100$)
5. Taking the inverse of the normalization function used in the temperature to convert it to Celcius. $f(t) = \frac{t-(-8)}{39-(-8)}$, $f^{-1}(t) = t*(39-(-8))+8$
6. Taking the inverse of the normalization function used for the temperature to convert it to (m/s) from $f(w)=\frac{1}{67}w (km/h)$, $f^{-1}(w)=\frac{w*67*1000}{60^2}$
7. Changing the Seasons from a numeric into an ordered factor in the same order as the Seoul Dataset ("Spring","Summer","Autumn","Winter")
8. Parsing date, putting it into similar FullDate = day-month-year-hour format and removing unneeded Date and Hour variables.
```{r washington_dataset}
washington_dataset <- washington_dataset %>%
  select(c('dteday','cnt','hr','temp','hum','windspeed','season','holiday')) %>%
  rename(Date = 'dteday', Count = 'cnt', Hour = 'hr', Temperature = 'temp', Humidity = 'hum', WindSpeed = 'windspeed', Season = 'season', Holiday = 'holiday') %>%
  filter(!is.na('Count')) %>%
  mutate(Humidity = Humidity*100) %>%
  mutate(Temperature = Temperature*(39-(-8))+(-8)) %>%
  mutate(WindSpeed = WindSpeed*67*1000/60^2) %>%
  mutate(Season = ifelse(Season == 1, "Winter", Season)) %>%
  mutate(Season = ifelse(Season == 2, "Spring", Season)) %>%
  mutate(Season = ifelse(Season == 3, "Summer", Season)) %>%
  mutate(Season = ifelse(Season == 4, "Autumn", Season)) %>%
  mutate(Season = ordered(Season, levels = c("Spring","Summer","Autumn","Winter"))) %>%
  mutate(Holiday = factor(ifelse(Holiday == 1, "Yes", "No"), levels = c("Yes","No"))) %>%
  mutate(Date = parse_date_time(Date,"ymd")) %>%
  mutate(FullDate = make_datetime(year = year(Date), month = month(Date), day = day(Date), hour = Hour)) %>%
  select(-c(Date,Hour))
```


## Creating big dataset with both Seoul and Washington DC
In order to more easily plot things with facet_wrap, I put all the data into one dataset and added a variable called "City" to denote which city it's coming from. 
```{r big_dataset}
seoul_dataset <- seoul_dataset %>%
  mutate(City = "Seoul")
washington_dataset <- washington_dataset %>%
  mutate(City = "Washington DC")
dataset <- seoul_dataset %>%
  add_row(washington_dataset) %>%
  arrange(Count)
```

# Plotting
In order to gain a better understanding of how each of the variables are related to eachother, we will plot different variables against eachother. 

## Temperature v Day Plot
In order to get a general idea of how the climates in each location change over the seasons, we will plot for both cities a scatter plot of the date versus the temperature for each datapoint (subdivided into hours). In order to plot this and include datasets that have a very different date range, I plotted against the day of the year, starting from January 1st. Additionally, I used stat_smooth to fit a line to estimate the expected temeperature for each day over the year. 
```{r temp_v_day}
temp_plot <- dataset %>%
  ggplot(aes(x = yday(FullDate), y = Temperature)) + 
  geom_point() + 
  xlab("Day of year from January 1st") +
  ylab("Temperature (C)")+
  stat_smooth(data = dataset, mapping = aes(x = yday(FullDate), y = Temperature)) +
  facet_wrap(~City, scales = "free") +
  ggtitle("Temperature over the year in Washington DC and Seoul")
temp_plot
```

## Plotting Season Against Bike Demand
Now that we have explored the temperature data across the year, it's time to start looking at how each of the explanatory variables affect the number of bikes rented per day. First we'll explore how the season of the year affects how many bikes are rented each day. To visualize this, I plotted a boxplot for each city. There's a clear increase in bike usage during the warmer seasons than in the winter in both cities. There is a more drastic drop in bike usage in Seoul during the winter than in Washington DC, and a question to explore would be whether or not this is due to Washington DC having a comparatively milder winter than Seoul. 
```{r season_v_bike_demand, message=FALSE, warning=FALSE}
season_plot_day <- dataset %>%
  group_by(date(FullDate),City, Season) %>%
  summarise(DayCount = sum(Count)) %>%
  ggplot(aes(x = Season, y = DayCount)) +
  geom_boxplot() + 
  facet_wrap(~City, scales = "free") + 
  ggtitle("Boxplot of Season Versus number of Bikes Rented per Day") +
  xlab("Season") +
  ylab("Number of bikes rented per day") 
season_plot_day
```

## Plotting holiday v bike demand
Next we would like to explore the affect of Holidays on bike usage. It appears at first that Seoul has a much larger affect on its bike rental usage. Looking further into this, this can be partially explained that for both cities, it appears the largest gap between Holidays and non-holidays is in the Winter, and Seoul has many more of its holidays on winter days than Washington DC does. 
```{r holiday_v_bike_demand, message=FALSE}
holiday_plot <- dataset %>%
  group_by(date(FullDate), City, Holiday, Season) %>%
  summarise(DayCount = sum(Count)) %>%
  ggplot(aes(x = Holiday, y = DayCount)) +
  geom_boxplot() +
  facet_wrap(~City, scales = "free") +
  ggtitle("Boxplot of Holiday versus number of Bikes rented per day") +
  xlab("Holiday") +
  ylab("Number of bikes rented per day")
holiday_plot
holiday_plot_season <- holiday_plot +
  facet_wrap(c(vars(Season),vars(City)), scales = "free_y", nrow = 4, ncol = 2)
holiday_plot_season #THIS SHOULD BE A BAR PLOT!!!!!!!!!!
```

## Plotting weather variables v bike demand
```{r temp_v_bike_demand, message=FALSE, warning=FALSE}
air_temp_plot <- dataset %>%
  ggplot(aes(x = Temperature, y = Count)) +
  geom_point() +
  facet_wrap(~City, scale = "free") +
  stat_smooth(data = dataset, mapping = aes(x = Temperature, y = Count), formula = y ~ x) +
  ggtitle("Temperature vs Bike Rental") +
  ylab("Number of bikes hired per hour")
air_temp_plot
```
There are clearly way too many datapoints to interpret this data properly. In order to clean it up, we will instead plot the average bike count for each temperature. In order to accomplish this, first I'll need to compute the average:
```{r computing_avg_bike_count_per_temp, message = FALSE, warning = FALSE}
temp_count_average <- dataset %>%
  group_by(Temperature, City) %>%
  summarise(mn = mean(Count))
```
Now we can plot this versus the temperature:
```{r temp_v_avg_bike_demand, message = FALSE, warning = FALSE}
air_temp_avg_plot <- temp_count_average %>%
  ggplot(aes(x = Temperature, y = mn)) +
  geom_point() +
  facet_wrap(~City, scale = "free") +
  stat_smooth(method = lm) +
  ggtitle("Temperature vs Bike Rental") +
  ylab("Number of bikes hired per hour")
air_temp_avg_plot
```
This is much more clear! Now we can get a much better picture of what's going on. Both datasets appear to have a close to linear (outside of the extremes) positive relationship where as temperature goes up the average number of bikes rented increases as well. In the Seoul dataset, at very high temperatures (>30 degrees) there appears to be a negative relationship between ridership and temperature as it gets too hot. This may hold as well for Washington DC, but it is hard to tell as we have much less data. 


```{r humidity_v_bike_demand}
humidity_plot <- dataset %>%
  ggplot(aes(x = Humidity, y = Count)) +
  geom_point() +
  facet_wrap(~City, scale = "free") +
  stat_smooth(data = dataset, mapping = aes(x = Humidity, y = Count), formula = y ~ x) +
  ggtitle("Humidity vs Bike Rental") +
  ylab("Number of bikes hired per hour")
humidity_plot
```
```{r wind_v_bike_demand}
wind_plot <- dataset %>%
  ggplot(aes(x = WindSpeed, y = Count)) +
  geom_point() +
  facet_wrap(~City, scale = "free") +
  stat_smooth(data = dataset, mapping = aes(x = WindSpeed, y = Count), formula = y ~ x) +
  ggtitle("WindSpeed (m/s) vs Bike Rental") +
  ylab("Number of bikes hired per hour")
wind_plot
```


# STATISTICAL MODELLING
Now that we've explored some of the relationships between the variables and Bike Rentals individually, it's now time to put all of it together into a linear model! For each of the models, we will plot a line that predicts the number of bike rentals based on the explanatary variables. $\log(\hat y) = \beta_0+\beta_1\mathbf{x}_{Winter}+\beta_2\mathbf x_{Spring}+\beta_3\mathbf x_{Summer}+\beta_4\mathbf x_{Temperature}+\beta_5\mathbf x_{Humidity}+\beta_6\mathbf x_{WindSpeed}$ Where $\hat y$ is the expected value of bikes rented in that hour. 

For each of the models, there is a nominal variable Season. This does not have a numerical value, so in order to put them into the model, we create 3 new variables: Winter, Spring and Summer (Autumn is the 'default'). For each of these variables we assign a value of 1 to it if it was the current season or else a 0 if it was a different season. 

## Linear Model for Seoul
Here we used `lm(formula = ..., data = ...)` to fit a model to the Seoul data. 
```{r linear_seoul}
seoul_model_dataset <- seoul_dataset %>%
  mutate(Winter = ifelse(Season == "Winter", 1, 0)) %>%
  mutate(Spring = ifelse(Season == "Spring", 1, 0)) %>%
  mutate(Summer = ifelse(Season == "Summer", 1, 0))

seoul_model <- lm(formula = log(Count) ~ Spring + Summer + Winter + Temperature + Humidity + WindSpeed, data = seoul_model_dataset)
summary(seoul_model)
```



## Linear model for Washington DC
```{r linear_washington}
washington_model_dataset <- washington_dataset %>%
  mutate(Winter = ifelse(Season == "Winter", 1, 0)) %>%
  mutate(Spring = ifelse(Season == "Spring", 1, 0)) %>%
  mutate(Summer = ifelse(Season == "Summer", 1, 0))


washington_model <- lm(formula = log(Count) ~ Winter + Spring + Summer + Temperature + Humidity + WindSpeed, data = washington_model_dataset)
summary(washington_model)

```
## Model Analysis
A linear model is built on the assumptions that the errors of the model are independent and identically normally distributed with mean 0 and variance $\sigma^2$. 

Here we save the residuals as a variable for ease of use in model checking. 
```{r residual_saving}
washington_residuals <- resid(washington_model)
seoul_residuals <- resid(seoul_model)
```
Now that we have the reiduals saved we will use base R's `plot.lm()` function to give us a few diagnostic tests for each of the models.
### Washington Model
Here we will go through the 4 given plots to check our assumptions for the linear model.

* *Residuals vs Fitted*: This checks whether or not this model is a linear relationship. The fitted line looks more or less linear, so we can assume for now that this is an appropriate model to use.
* *Normal Q-Q*: This checks whether or not the residuals are normally distributed. As the line is close to linear, this seems to be a reasonable assumption to make for our model. 
* *Scale-Location*: This checks to see if our assumption of equal variance for residuals holds. If the line is horizontal and the spread seems uniform between than this assumption holds. In this plot we have a clear downward trend, so would be worth investigating further if our assumption of equal variance is appropriate.
* *Residuals vs Leverage*: This plot gives us information if there's any outliers having a significant impact on our model. As our Cook Distance lines are barely visible, most of our data lies within the lines or very close to it. Therefore we can take the assumption that our model isn't affected greatly by outliers. 
```{r model_analysis_washington}
par(mfrow = c(2,2))
plot(washington_model)
```

### Seoul Model
We will again go through the 4 plots for this model:

* *Residuals vs Fitted*: As above, the assumption that this model is linear seems appropriate.
* *Normal Q-Q*: Outside of the tails this line is straight so the residuals are probably close to linearly distributed.
* *Scale-Location*: The assumption seems reasonable that the residuals have equal variance.
* *Residuals vs Leverage*: There doesn't appear to be any outliers that are greatly affecting our model.
```{r model_analysis_seoul}
par(mfrow = c(2,2))
plot(seoul_model)
```

## Confidence Intervals for Models
Here, I've generated a 97% confidence interval for each of the variables. The Washington DC model differs pretty greatly from the model for Seoul in that it has negative coefficients for for all of the Seasons and has a greater coefficient for temperature in order to compensate for this difference. 
```{r confidence_intervals}
confint(seoul_model, level = 0.97) 
confint(washington_model, level = 0.97)

example_predict <- data.frame(Temperature = 0,
                              WindSpeed = 0.5,
                              Humidity = 20,
                              Winter = 1,
                              Spring = 0,
                              Summer = 0)
log_seoul_predict <- predict(seoul_model, interval = "prediction", level = 0.90, newdata = example_predict)
log_washington_predict <- predict(washington_model, interval = "prediction", level = 0.90, newdata = example_predict)
```

