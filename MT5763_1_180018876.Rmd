---
title: "Bike Share Analysis"
author: "Matthew Coudert"
date: "Saturday September 26th"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
Bike Shares have been rising in popularity over the past few years in many cities all around the world. In order to predict usage of bikes throughout the year, we will explore the relationships between bike usage and other variables, such as day of the year, season, temperature and other meteorlogical variables. After exploring these relationships, we will use this knowledge to build a model in order to predict bike demand throughout the year. For this project, we have data from Washington DC and Seoul, and will use these data to build two distinct models. 

## Downloading Packages Needed
In order to assist us with the data visualization and clean up, we will download 3 packages:

1. **tidyverse**: Assists with putting the data into 'tidy' format
2. **ggplot2**: Makes plotting cleaner and more readable, as well as adding functionality
3. **lubridate**: Makes dates easier to work with in our datasets
```{r packages}
library(tidyverse)
library(ggplot2)
library(lubridate)
```

# Data Cleanup
In order to do analysis on the data, we first need to 'tidy' the data in order to have it in a format that will be easier to work with as well as being in the same format across both datasets for comparison. In order to accomplish this we will put the data into 'tidy' format using the package tidyverse. In order for data to be 'tidy', it must have: "Rows containing differrent observations; columns containing different variables and cells containing values."


## Saving datasets
The messy datasets are saved in the csv format for easy parsing. 
```{r datasets, message = FALSE, warning = FALSE, results = FALSE}
washington_dataset <- read_csv("BikeWashingtonDC.csv")
seoul_dataset <- read_csv("BikeSeoul.csv")
```

## Seoul dataset clean up
This is a lot! I'll comment each of the lines in order to explain what I'm doing.
```{r seoul_dataset}
seoul_dataset <- seoul_dataset %>%
  filter(seoul_dataset$`Functioning Day`=="Yes") %>% #Filtering out all the days that a count wasn't kept track of
  select(-c('Visibility (10m)','Dew point temperature(C)','Solar Radiation (MJ/m2)','Rainfall(mm)','Snowfall (cm)','Functioning Day')) %>% #Removing all the unneeded variables in the dataset
  filter(!is.na('Rented Bike Count')) %>% #Remove hours with NA for the bike Count
  rename(Date = 'Date',Count = 'Rented Bike Count',Hour = 'Hour', Temperature = 'Temperature(C)', Humidity = 'Humidity(%)', WindSpeed = 'Wind speed (m/s)', Season = 'Seasons', Holiday = 'Holiday') %>% #Renaming variables for consistency
  mutate(Date = parse_date_time(Date,"dmy")) %>% #Parsing the dates with Lubridate
  mutate(FullDate = make_datetime(year = year(Date), month = month(Date), day = day(Date), hour = Hour)) %>%
  select(-c(Date,Hour)) %>% #Removing the redundant Date and Hour variables
  mutate(Holiday = factor(ifelse(Holiday == "Holiday", "Yes", "No"), levels = c("Yes","No"))) %>% #Changing Season and Holiday to a factor
  mutate(Season = factor(Season, levels = c("Spring","Summer","Autumn","Winter")))
```

## Washington DC dataset clean up
Similar to above, using the `tidyverse` package and piping to put the data into tidy format with consistent naming conventions and formats to the Seoul dataset. 
```{r washington_dataset}
washington_dataset <- washington_dataset %>%
  select(c('dteday','cnt','hr','temp','hum','windspeed','season','holiday')) %>% #Selecting for the variables we need
  rename(Date = 'dteday', Count = 'cnt', Hour = 'hr', Temperature = 'temp', Humidity = 'hum', WindSpeed = 'windspeed', Season = 'season', Holiday = 'holiday') %>% #Renaming all the variables for consistency with other dataset
  filter(!is.na('Count')) %>%
  mutate(Humidity = Humidity*100) %>% #Converting humidity into a percent
  mutate(Temperature = Temperature*(39-(-8))+(-8)) %>% #Taking inverse of temperature normalization function to have all temperature data in Celcius
  mutate(WindSpeed = WindSpeed*67*1000/60^2) %>% #Taking inverse of the WindSpeed normalization function and converting it into (m/s)
  mutate(Season = ifelse(Season == 1, "Winter", Season)) %>% #Changing the season from a numeric to a factor
  mutate(Season = ifelse(Season == 2, "Spring", Season)) %>%
  mutate(Season = ifelse(Season == 3, "Summer", Season)) %>%
  mutate(Season = ifelse(Season == 4, "Autumn", Season)) %>%
  mutate(Season = factor(Season, levels = c("Spring","Summer","Autumn","Winter"))) %>% 
  mutate(Holiday = factor(ifelse(Holiday == 1, "Yes", "No"), levels = c("Yes","No"))) %>% #Changing Holiday to a factor
  mutate(Date = parse_date_time(Date,"ymd")) %>% # Using Lubridate to parse the date
  mutate(FullDate = make_datetime(year = year(Date), month = month(Date), day = day(Date), hour = Hour)) %>%
  select(-c(Date,Hour)) #Removing Date and Hour now that we have a FullDate variable as they are redundant
```

## Creating big dataset with both Seoul and Washington DC
In order to more easily plot things with facet_wrap, I put all the data into one dataset and added a variable called "City" to denote which city it's coming from. 
```{r big_dataset}
seoul_dataset <- seoul_dataset %>%
  mutate(City = "Seoul")
washington_dataset <- washington_dataset %>%
  mutate(City = "Washington DC")
dataset <- seoul_dataset %>%
  add_row(washington_dataset) %>%
  arrange(Count)
```
Great! Now both our datasets are in tidy format and we're ready to move on to visualize our data!

# Plotting
In order to gain a better understanding of how each of the variables are related to eachother, we will plot different variables against eachother. In order to accomplish this we will use `ggplot`. A `tidyverse` package that makes plotting easy to follow. 

## Temperature v Day Plot
In order to get a general idea of how the climates in each location change over the seasons, we will plot for both cities a scatter plot of the date versus the temperature for each datapoint (subdivided into hours). In order to plot this and include datasets that have a very different date range, I plotted against the day of the year, starting from January 1st. Additionally, I used stat_smooth to fit a line to estimate the expected temeperature for each day over the year. 
```{r temp_v_day}
temp_plot <- dataset %>%
  ggplot(aes(x = yday(FullDate), y = Temperature)) +  #Initializing the plot with the X variable date and Y variable temperature
  geom_point() + #Making it a scatter plot
  xlab("Day of year from January 1st") + #Labelling axis
  ylab("Temperature (C)")+
  stat_smooth(data = dataset, mapping = aes(x = yday(FullDate), y = Temperature)) + #Adding fit line
  facet_wrap(~City, scales = "free") + #Plotting the 2 plots next to eachother with different scales for each 
  ggtitle("Temperature over the year in Washington DC and Seoul") #Adding a title
temp_plot
```

## Season vs Bike Usage
Now that we have explored the temperature data across the year, it's time to start looking at how each of the explanatory variables affect the number of bikes rented per day. First we'll explore how the season of the year affects how many bikes are rented each day. To visualize this, I plotted a boxplot for each city. There's a clear increase in bike usage during the warmer seasons than in the winter in both cities. There is a more drastic drop in bike usage in Seoul during the winter than in Washington DC, and a question to explore would be whether or not this is due to Washington DC having a comparatively milder winter than Seoul. 
```{r season_v_bike_demand, message=FALSE, warning=FALSE}
season_plot_day <- dataset %>%
  group_by(date(FullDate),City, Season) %>%
  summarise(DayCount = sum(Count)) %>% #Getting the sum of each days total and piping that into the plot
  ggplot(aes(x = Season, y = DayCount)) +
  geom_boxplot() + #Making it a boxplot
  facet_wrap(~City, scales = "free") + 
  ggtitle("Season vs Bike Usage") +
  xlab("Season") +
  ylab("Number of bikes rented per day") 
season_plot_day
```

## Holiday vs Bike Usage
Next we would like to explore the affect of Holidays on bike usage. 
```{r holiday_v_bike_demand, message=FALSE}
holiday_plot <- dataset %>%
  group_by(date(FullDate), City, Holiday, Season) %>%
  summarise(DayCount = sum(Count)) %>% #Again taking sum of each days total rather than going by hour
  ggplot(aes(x = Holiday, y = DayCount)) +
  geom_boxplot() +
  facet_wrap(~City, scales = "free") +
  ggtitle("Affect of Holidays on Bike Usage") +
  xlab("Whether or not a given day is a Holiday") +
  ylab("Number of bikes rented per day")
holiday_plot
```
It appears at first that Seoul has a much larger affect on its bike rental usage. Looking further into this, this can be partially explained that for both cities, it appears the largest gap between Holidays and non-holidays is in the Winter, and Seoul has many more of its holidays on Winter days than Washington DC does combined with the fact that Winter has on average less bike usage than other seasons.

## Time of Day vs Bike Usage
In order to plot the time of day vs the number of bikes hour we will first find the average number of bikes hired for each dataset:
```{r computing_avg_bike_count_per_hour, message = FALSE}
hour_count_average <- dataset %>%
  group_by(Hour = hour(FullDate), City) %>%
  summarise(mn = mean(Count)) #Taking the mean of each day
```
Now we can plot this relationship:
```{r hour_v_avg_bike_demand}
hour_avg_plot <- hour_count_average %>%
  ggplot(aes(x = Hour, y = mn)) +
  geom_col() +
  facet_wrap(~City, scale = "free") +
  ggtitle("Time of Day vs Bike Usage") +
  ylab("Number of bikes hired per hour") +
  xlab("Time of Day")
hour_avg_plot
```
There's clearly an increase at commuting times of 9 and 16-18 hours with a sharp decrease in the late hours of the night. Considering these are very common commuting times it makes sense that the peak would be during these times of day. 


## Weather Variables vs Bike Usage
In order to accurately predict bike usage, we need to look at more than just the date and time. We also need to look at the affect of different meteorlogical variables. 

### Temperature vs Bike Usage
```{r temp_v_bike_demand, message=FALSE, warning=FALSE}
air_temp_plot <- dataset %>%
  ggplot(aes(x = Temperature, y = Count)) +
  geom_point() +
  facet_wrap(~City, scale = "free") +
  stat_smooth(data = dataset, mapping = aes(x = Temperature, y = Count), formula = y ~ x) +
  ggtitle("Temperature vs Bike Usage") +
  ylab("Number of Bikes hired per Hour") +
  xlab("Temperature")
air_temp_plot
```
While we can see there is clearly a positive linear relationship between bike rentals and temperature, there are clearly way too many datapoints for us to interpret these data properly in this format. In order to make the plot more readable, we will instead plot the average bike count for each temperature. In order to accomplish this, first I'll need to compute the average:
```{r computing_avg_bike_count_per_temp, message = FALSE, warning = FALSE}
temp_count_average <- dataset %>%
  group_by(Temperature, City) %>% #Sorting the data by both Temperature and the City it's from
  summarise(mn = mean(Count)) #Taking the mean for each Temperature/City combination
```
Now we can plot this mean versus the temperature:
```{r temp_v_avg_bike_demand, message = FALSE, warning = FALSE}
air_temp_avg_plot <- temp_count_average %>%
  ggplot(aes(x = Temperature, y = mn)) +
  geom_point() +
  facet_wrap(~City, scale = "free") +
  stat_smooth(method = lm) +
  ggtitle("Temperature vs Bike Usage") +
  ylab("Number of bikes hired per hour")
air_temp_avg_plot
```
Now we can get a much better picture of what's going on! Both datasets appear to have a close to linear (outside of the extremes) positive relationship where as temperature goes up the average number of bikes rented increases as well. In the Seoul dataset, at very high temperatures (>30 degrees) there appears to be a negative relationship between ridership and temperature as temperature increases. This may hold as well for Washington DC, but it is hard to tell as we have much less data for high temperatures in Washington than we do for Seoul. 

### Humidity vs Bike Usage
Here we have the same issue as above, in that there is too much data to properly visualize so we will again compute the average count for each humidity value:
```{r computing_avg_bike_count_per_hum, message = FALSE, warning = FALSE}
hum_count_average <- dataset %>%
  group_by(Humidity, City) %>% 
  summarise(mn = mean(Count))
```
Now we can plot this to see what the relationship is:
```{r humidity_v_bike_demand}
humidity_plot <- hum_count_average %>%
  ggplot(aes(x = Humidity, y = mn)) +
  geom_point() +
  facet_wrap(~City, scale = "free") +
  stat_smooth(method = 'lm') +
  ggtitle("Humidity vs Bike Usage") +
  ylab("Number of bikes hired per hour") +
  xlab("Humidity (%)")
humidity_plot
```
For both cities there appears to be a clear negative relationship between humidity and ridership. In Washington DC's model there's additionally some outliers with very low humidity have very low ridership and in Seoul's model there is the opposite, with outliers falling way above this model. This could be partially explained by that a very small proportion of both models have datapoints with humidity less than 20, so any singular outlier could have a large effect on the average for that humidity.  

### Wind v Bike Usage
Again we will group and average the counts by Windspeed and City:
```{r computing_avg_bike_count_per_wind, message = FALSE, warning = FALSE}
wind_count_average <- dataset %>%
  group_by(WindSpeed, City) %>% 
  summarise(mn = mean(Count))
```
And the plot: 
```{r wind_v_bike_demand}
wind_plot <- wind_count_average %>%
  ggplot(aes(x = WindSpeed, y = mn)) +
  geom_point() +
  facet_wrap(~City, scale = "free") +
  stat_smooth(method = 'lm') +
  ggtitle("WindSpeed (m/s) vs Bike Usage") +
  ylab("Number of bikes hired per hour") +
  xlab("Wind Speed (m/s)")
wind_plot
```
For both of these datasets, it looks quite clearly non-linear with a peak of around 2.5(m/s) for Seoul and 5(m/s). This could be due to the fact that it is either non-linear or the fact that there is less data for larger or smaller wind speeds increasing the affect of outliers on the model.

# STATISTICAL MODELLING
Now that we've explored some of the relationships between the variables and Bike Rentals individually, it's now time to put all of it together into a linear model! For each of the models, we will plot a line that predicts the number of bike rentals based on the explanatary variables. $\log(\hat y) = \beta_0+\beta_1\mathbf{x}_{Summer}+\beta_2\mathbf x_{Autumn}+\beta_3\mathbf x_{Winter}+\beta_4\mathbf x_{Temperature}+\beta_5\mathbf x_{Humidity}+\beta_6\mathbf x_{WindSpeed}$ Where $\hat y$ is the expected value of bikes rented in that hour. 

## Linear Model for Seoul
Here we used `lm(formula = ..., data = ...)` to fit a model to the Seoul data. 
```{r linear_seoul}
seoul_model_dataset <- seoul_dataset
seoul_model <- lm(formula = log(Count) ~ Season + Temperature + Humidity + WindSpeed, data = seoul_model_dataset)
summary(seoul_model)
```


## Linear model for Washington DC
```{r linear_washington}
washington_model <- lm(formula = log(Count) ~ Season + Temperature + Humidity + WindSpeed, data = washington_model_dataset)
summary(washington_model)
```
## Model Analysis
A linear model is built on the assumptions that the errors of the model are independent and identically normally distributed with mean 0 and variance $\sigma^2$. 

Here we save the residuals as a variable for ease of use in model checking. 
```{r residual_saving}
washington_residuals <- resid(washington_model)
seoul_residuals <- resid(seoul_model)
```
Now that we have the reiduals saved we will use base R's `plot.lm()` function to give us a few diagnostic tests for each of the models.
### Washington Model
Here we will go through the 4 given plots to check our assumptions for the linear model.
```{r model_analysis_washington}
par(mfrow = c(2,2))
plot(washington_model)
```

* *Residuals vs Fitted*: This checks whether or not this model is a linear relationship. The fitted line looks more or less linear, so we can assume for now that this is an appropriate model to use.
* *Normal Q-Q*: This checks whether or not the residuals are normally distributed. As the line is close to linear, this seems to be a reasonable assumption to make for our model. 
* *Scale-Location*: This checks to see if our assumption of equal variance for residuals holds. If the line is horizontal and the spread seems uniform between than this assumption holds. In this plot we have a clear downward trend, so would be worth investigating further if our assumption of equal variance is appropriate.
* *Residuals vs Leverage*: This plot gives us information if there's any outliers having a significant impact on our model. As our Cook Distance lines are barely visible, most of our data lies within the lines or very close to it. Therefore we can take the assumption that our model isn't affected greatly by outliers. 

### Seoul Model
We will again go through the 4 plots for this model:
```{r model_analysis_seoul}
par(mfrow = c(2,2))
plot(seoul_model)
```

* *Residuals vs Fitted*: As above, the assumption that this model is linear seems appropriate.
* *Normal Q-Q*: Outside of the tails this line is straight so the residuals are probably close to linearly distributed.
* *Scale-Location*: The assumption seems reasonable that the residuals have equal variance.
* *Residuals vs Leverage*: There doesn't appear to be any outliers that are greatly affecting our model.

## Confidence Intervals for Models
Here, I've generated a 97% confidence interval for each of the variables. The Washington DC model differs pretty greatly from the model for Seoul in that it has negative coefficients for for all of the Seasons and has a greater coefficient for temperature in order to compensate for this difference. 
```{r confidence_intervals}
confint(seoul_model, level = 0.97) 
confint(washington_model, level = 0.97)
```

## Using our Models!
Now for the rest of the project, we'll assume our model validates the assumptions above and is a good fit for the data. Here we will take a Winter day with 0(C) Temperature, a Wind Speed of 0.5(m/s), a Humidity of 20% and predict how many bikes will be rented in a given hour with those specifications. We will use a confidence interval of 90%. 
```{r logged_prediction}
example_predict <- data.frame(Temperature = 0,
                              WindSpeed = 0.5,
                              Humidity = 20,
                              Season = 'Winter')
log_seoul_predict <- predict(seoul_model, interval = "prediction", level = 0.90, newdata = example_predict)
log_washington_predict <- predict(washington_model, interval = "prediction", level = 0.90, newdata = example_predict)
print(log_seoul_predict)
print(log_washington_predict)
```
We have to remember this is a prediction of the $log$ of the count so in order to get a prediction for the number of bikes that will be rented in that hour we'll have to take $e^{\hat y}$ where $\hat y$ is the output of the model.
```{r prediction}
print(exp(log_seoul_predict))
print(exp(log_washington_predict))
```
The expected value for Seoul is 369.96 bikes rented in an hour with those specifications with an upper and lower bound of $(94.75,1444.63)$ with 90% confidence. The expected value for Washington DC is 71.98 and has bounds $(9.01,575.29)$. For both models we have a very large prediction interval with 90% confidence. This suggests there is a high level of variance and that the model isn't very accurate. Perhaps a linear model doesn't account for all the nuances of the data, such as the perhaps non-linear relationship between temperature and bike usage. 